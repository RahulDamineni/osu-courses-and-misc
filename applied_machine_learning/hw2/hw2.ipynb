{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"../hw1/hw1-data\")\n",
    "\n",
    "train_path = DATA_DIR / \"income.train.txt.5k\"\n",
    "eval_path = DATA_DIR / \"income.dev.txt\"\n",
    "test_path = DATA_DIR / \"income.test.blind\"\n",
    "train_and_eval_path = DATA_DIR / \"income.combined.6k\"\n",
    "\n",
    "COL_NAMES = [\n",
    "    \"age\",\n",
    "    \"sector\",\n",
    "    \"education\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"hours-per-week\",\n",
    "    \"country-of-origin\",\n",
    "    \"target\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_to_df(file_path):\n",
    "        \n",
    "    DELIMITER = \",\"\n",
    "    \n",
    "    def parse_row(row, delimiter=DELIMITER):\n",
    "        cells = row.split(delimiter)\n",
    "        parsed_row = [\n",
    "            int(cell.strip()) \n",
    "            if cell.isnumeric() \n",
    "            else cell.strip()\n",
    "            for cell in cells \n",
    "        ]\n",
    "        \n",
    "        return parsed_row\n",
    "        \n",
    "    data = []\n",
    "    with open(file_path) as in_:\n",
    "        raw_rows = in_.readlines()\n",
    "        \n",
    "    for row in raw_rows:\n",
    "        parsed_row = parse_row(row)\n",
    "        data.append(parsed_row)\n",
    "    \n",
    "    df = {col: val for col, val in zip(*[COL_NAMES, zip(*data)])}\n",
    "    return df, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50,\n",
       "  'Self-emp-not-inc',\n",
       "  'Bachelors',\n",
       "  'Married-civ-spouse',\n",
       "  'Exec-managerial',\n",
       "  'White',\n",
       "  'Male',\n",
       "  '13',\n",
       "  'United-States',\n",
       "  '<=50K'],\n",
       " dict_keys(['age', 'sector', 'education', 'marital-status', 'occupation', 'race', 'gender', 'hours-per-week', 'country-of-origin', 'target']))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First person in eval\n",
    "df, data = txt_file_to_df(train_path)\n",
    "data[0], df.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Takes file name and trained encoders as input to \n",
    "load, parse and transform dataset into modelling ready dataframe\n",
    "'''\n",
    "\n",
    "def txt_to_encoded_df(file_path, encoders):\n",
    "\n",
    "    # Load and parse data\n",
    "    df, _ = txt_file_to_df(file_path)\n",
    "\n",
    "    # Encode and transform each col\n",
    "    encoded_df = []\n",
    "    for col in df.keys():\n",
    "\n",
    "        if col == \"target\":\n",
    "            continue\n",
    "\n",
    "        elif col in encoders:\n",
    "            encoder = encoders[col]\n",
    "            col_values = encoder.transform(df[col])\n",
    "\n",
    "        else:\n",
    "            col_values = np.array(df[col], dtype=np.float64).reshape(len(df[col]), -1) / 50.\n",
    "\n",
    "        encoded_df.append(col_values)\n",
    "\n",
    "    # Make a flat dataset from all cols\n",
    "    encoded_df = np.hstack(encoded_df)\n",
    "\n",
    "    if \"test\" in str(file_path):\n",
    "        return encoded_df\n",
    "\n",
    "    return encoded_df, df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder\n",
    "\n",
    "class OneHotEncoder(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, column):\n",
    "        self.unique_categories = list(set(column))\n",
    "        self.catg_index = {catg: cid for cid, catg in enumerate(self.unique_categories)}\n",
    "        \n",
    "    def transform(self, column):\n",
    "        ct, ck = 0, []\n",
    "        self.ohe_column = np.zeros((len(column), len(self.unique_categories)), dtype=np.float64)\n",
    "        for i, catg in enumerate(column):\n",
    "            try:\n",
    "                j = self.catg_index[catg]\n",
    "                self.ohe_column[i, j] = 1\n",
    "            except KeyError:\n",
    "                ct += 1\n",
    "                ck.append(catg)\n",
    "                \n",
    "        print(f'Failed {ct} times because of {ck}')\n",
    "        return self.ohe_column\n",
    "    \n",
    "    def fit_transform(self, column):\n",
    "        self.fit(column=column)\n",
    "        \n",
    "        return self.transform(column=column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sector': <__main__.OneHotEncoder at 0x1216bdcd0>,\n",
       " 'education': <__main__.OneHotEncoder at 0x121723b90>,\n",
       " 'marital-status': <__main__.OneHotEncoder at 0x1216d1a50>,\n",
       " 'occupation': <__main__.OneHotEncoder at 0x123fc8510>,\n",
       " 'race': <__main__.OneHotEncoder at 0x123fe3050>,\n",
       " 'gender': <__main__.OneHotEncoder at 0x1241d3390>,\n",
       " 'country-of-origin': <__main__.OneHotEncoder at 0x124262910>,\n",
       " 'hours-per-week': <__main__.OneHotEncoder at 0x12426eed0>,\n",
       " 'age': <__main__.OneHotEncoder at 0x1216eb650>}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding categorical vars\n",
    "\n",
    "def make_encoders(categorical_columns, train_df):\n",
    "    '''\n",
    "    Takes categorical column names and df to make encoder objects using data\n",
    "    '''\n",
    "\n",
    "    catg_columns = categorical_columns.keys()\n",
    "\n",
    "    # Init encoder objects\n",
    "    col_encoders = {col: OneHotEncoder() for col in catg_columns}\n",
    "    \n",
    "    # Train encoders\n",
    "    for col in catg_columns:\n",
    "        col_values = col_encoders[col].fit(train_df[col]) \n",
    "        \n",
    "    return col_encoders\n",
    "\n",
    "\n",
    "cols_catg = {\n",
    "    \"sector\": 7,\n",
    "    \"education\": 16,\n",
    "    \"marital-status\": 4,\n",
    "    \"occupation\": 14,\n",
    "    \"race\": 5,\n",
    "    \"gender\": 2,\n",
    "    \"country-of-origin\": 39,\n",
    "    \"hours-per-week\": \"*\",\n",
    "    \"age\": \"*\"\n",
    "}\n",
    "\n",
    "combined_df, _ = txt_file_to_df(train_and_eval_path)\n",
    "\n",
    "make_encoders(categorical_columns=cols_catg, train_df=combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "(5000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of this dataset's ETL cycle\n",
    "\n",
    "combined_df_raw, _ = txt_file_to_df(train_and_eval_path)\n",
    "one_hot_encoders = make_encoders(categorical_columns=cols_catg, train_df=combined_df_raw)\n",
    "train_df = txt_to_encoded_df(file_path=train_path, encoders=one_hot_encoders)\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 1 times because of [83]\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 2 times because of ['31', '29']\n",
      "Failed 0 times because of []\n"
     ]
    }
   ],
   "source": [
    "combined_df_raw, _ = txt_file_to_df(train_and_eval_path)\n",
    "one_hot_encoders = make_encoders(categorical_columns=cols_catg, train_df=combined_df_raw)\n",
    "\n",
    "train_df = txt_file_to_df(train_path)[0]\n",
    "eval_df = txt_file_to_df(eval_path)[0]\n",
    "test_df = txt_file_to_df(test_path)[0]\n",
    "\n",
    "train_X, train_y = txt_to_encoded_df(file_path=train_path, encoders=one_hot_encoders)\n",
    "eval_X, eval_y = txt_to_encoded_df(file_path=eval_path, encoders=one_hot_encoders)\n",
    "test_X = txt_to_encoded_df(file_path=test_path, encoders=one_hot_encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why binarize numerical fields?\n",
    "KNN uses distance metric while Perceptron uses dot product.\n",
    "Binarizing age would help optimization algorithm to preserve which ages are \"useful\" while updating W.\n",
    "This usefulness would be measured by dot product. If we hadn't binarized, dot product won't have this granular distingushing power but more liekly saturate.\n",
    "\n",
    "### Total number of features?\n",
    "234 Including bias and a couple of feature that are only present in dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron and Averaged Perceptron\n",
    "\n",
    "### Basic perceptron algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, epochs, n_features):\n",
    "        self.n_epochs = epochs\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Integrating bias into W\n",
    "        self.W = np.zeros(self.n_features + 1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_examples = X.shape[0]\n",
    "        \n",
    "        updates = 0\n",
    "        for rid in range(self.n_examples):\n",
    "            row = X[rid, :]\n",
    "            target = y[rid]\n",
    "            \n",
    "            if np.dot(self.W, row) * target <= 0:\n",
    "                self.W += target * row\n",
    "                updates += 1\n",
    "                \n",
    "        return updates\n",
    "    \n",
    "    @staticmethod\n",
    "    def binarize_y(y):\n",
    "        y_bin = np.array(y)\n",
    "        y_bin[y_bin == \"<=50K\"] = -1\n",
    "        y_bin[y_bin != \"-1\"] = 1\n",
    "        y_bin = y_bin.astype(np.int64)\n",
    "\n",
    "        return y_bin\n",
    "    \n",
    "    @staticmethod\n",
    "    def expand_bias_dim(X):\n",
    "        bias_dim = np.ones((X.shape[0], 1))\n",
    "        \n",
    "        return np.hstack((bias_dim, X))\n",
    "        \n",
    "                    \n",
    "    \n",
    "    def evaluate(self, teX, tey):\n",
    "        predictions = (np.dot(teX, self.W) > 0).astype(np.float)\n",
    "        predictions[predictions == 0] = -1\n",
    "        \n",
    "        err_rate = np.mean(predictions != tey)\n",
    "        pos_rate = np.mean(predictions == 1)\n",
    "        \n",
    "        return err_rate, pos_rate\n",
    "    \n",
    "    def summary(self, trX, trY, teX, teY):\n",
    "        \n",
    "        trX = self.expand_bias_dim(trX)\n",
    "        trYb = self.binarize_y(trY)\n",
    "        teX = self.expand_bias_dim(teX)\n",
    "        teYb = self.binarize_y(teY)\n",
    "        \n",
    "        for e in range(1, self.n_epochs + 1):\n",
    "            \n",
    "            n_updates = self.fit(trX, trYb)\n",
    "            perc_updates = n_updates * 100 / self.n_examples\n",
    "            \n",
    "            err_rate, pos_rate = self.evaluate(teX, teYb)\n",
    "            err_perc, pos_perc = err_rate * 100, pos_rate * 100\n",
    "            \n",
    "            print(\n",
    "            f'epoch {e} updates {n_updates} ({perc_updates:.2f}%) dev_error {err_perc:.2f}% (+:{pos_perc:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1257 (25.14%) dev_error 21.10% (+:27.50%)\n",
      "epoch 2 updates 1221 (24.42%) dev_error 18.80% (+:25.40%)\n",
      "epoch 3 updates 1177 (23.54%) dev_error 17.50% (+:21.50%)\n",
      "epoch 4 updates 1170 (23.40%) dev_error 19.10% (+:12.30%)\n",
      "epoch 5 updates 1172 (23.44%) dev_error 18.70% (+:17.70%)\n"
     ]
    }
   ],
   "source": [
    "pt = Perceptron(epochs=5, n_features=train_X.shape[1])\n",
    "pt.summary(trX=train_X, trY=train_y, teX=eval_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged perceptron algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPerceptron(object):\n",
    "    \n",
    "    def __init__(self, epochs, n_features):\n",
    "        self.n_epochs = epochs\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Integrating bias into W\n",
    "        self.W = np.zeros(self.n_features + 1)\n",
    "        \n",
    "        self.av_W = np.zeros(self.n_features + 1)\n",
    "        self.c = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_examples = X.shape[0]\n",
    "        \n",
    "        updates = 0\n",
    "        for rid in range(self.n_examples):\n",
    "            row = X[rid, :]\n",
    "            target = y[rid]\n",
    "            \n",
    "            if np.dot(self.W, row) * target <= 0:\n",
    "                self.W += target * row\n",
    "                self.av_W += self.c * target * row\n",
    "                updates += 1\n",
    "                \n",
    "            self.c += 1\n",
    "                \n",
    "        return updates\n",
    "    \n",
    "    @staticmethod\n",
    "    def binarize_y(y):\n",
    "        y_bin = np.array(y)\n",
    "        y_bin[y_bin == \"<=50K\"] = -1\n",
    "        y_bin[y_bin != \"-1\"] = 1\n",
    "        y_bin = y_bin.astype(np.int64)\n",
    "\n",
    "        return y_bin\n",
    "    \n",
    "    @staticmethod\n",
    "    def expand_bias_dim(X):\n",
    "        bias_dim = np.ones((X.shape[0], 1))\n",
    "        \n",
    "        return np.hstack((bias_dim, X))\n",
    "        \n",
    "                    \n",
    "    \n",
    "    def evaluate(self, teX, tey):\n",
    "        \n",
    "        effective_W = self.c * self.W - self.av_W\n",
    "        predictions = (np.dot(teX, effective_W) > 0).astype(np.float)\n",
    "        predictions[predictions == 0] = -1\n",
    "        \n",
    "        err_rate = np.mean(predictions != tey)\n",
    "        pos_rate = np.mean(predictions == 1)\n",
    "        \n",
    "        return err_rate, pos_rate\n",
    "    \n",
    "    def summary(self, trX, trY, teX, teY):\n",
    "        \n",
    "        trX = self.expand_bias_dim(trX)\n",
    "        trYb = self.binarize_y(trY)\n",
    "        teX = self.expand_bias_dim(teX)\n",
    "        teYb = self.binarize_y(teY)\n",
    "        \n",
    "        for e in range(1, self.n_epochs + 1):\n",
    "            \n",
    "            n_updates = self.fit(trX, trYb)\n",
    "            perc_updates = n_updates * 100 / self.n_examples\n",
    "            \n",
    "            err_rate, pos_rate = self.evaluate(teX, teYb)\n",
    "            err_perc, pos_perc = err_rate * 100, pos_rate * 100\n",
    "            \n",
    "            print(\n",
    "            f'epoch {e} updates {n_updates} ({perc_updates:.2f}%) dev_error {err_perc:.2f}% (+:{pos_perc:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1257 (25.14%) dev_error 15.00% (+:18.60%)\n",
      "epoch 2 updates 1221 (24.42%) dev_error 15.10% (+:19.30%)\n",
      "epoch 3 updates 1177 (23.54%) dev_error 14.80% (+:20.00%)\n",
      "epoch 4 updates 1170 (23.40%) dev_error 14.70% (+:19.30%)\n",
      "epoch 5 updates 1172 (23.44%) dev_error 14.80% (+:20.00%)\n"
     ]
    }
   ],
   "source": [
    "apt = AvgPerceptron(epochs=5, n_features=train_X.shape[1])\n",
    "apt.summary(trX=train_X, trY=train_y, teX=eval_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "Vanilla's updates seemed jumpy while Averaged Perceptron updates are much stabler. Also the dev error rate of AP is smaller at 14.70% compared to vanilla's 17.50%\n",
    "\n",
    "### Positive & Negative features\n",
    "\n",
    "#### Positives\n",
    "Married\n",
    "Education: doctorate, prof-school\n",
    "Country: Iran\n",
    "\n",
    "#### Negatives\n",
    "Ages: 26, 28\n",
    "Education: 7-8th \n",
    "Occupation: Farming-Fishing \n",
    "\n",
    "### Male, Female\n",
    "These features present in every example (both positive & negative) and would add +1 during dot product. \n",
    "To center the dot product around 0, these may have drifted towards negative values? \n",
    "\n",
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.0"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apt.W[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Is the update % above equivalent to “training error”?\n",
    "\n",
    "No, the update % is simply number of misclassifed examples on the rolling basis from previous epoch.\n",
    "The weight vector changes as we perform updates leaving no bechmark to calculate training error that would be consistent over any arrangement of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Perceptron with KNN\n",
    "\n",
    "### What are the major advantages of perceptron over k-NN?\n",
    "* Perceptron summarizes training data into weight vector which makes inference much faster (as KNN has to compare over all training data to classify when Perceptron has to compare over just W)\n",
    "* Perceptron uses \"dot product\" while KNN uses \"distance metric\": this translates to KNN attributing importance to vectors which may be completely orthogonal to dataset \"prime\" vector but are still equidistant. \n",
    "\n",
    "### Design and execute a small experiment to demonstrate your point(s).\n",
    "- KNN evaluation time: 5 seconds (k=41)\n",
    "- Perceptron evaluation time: ~<1 seconds\n",
    "\n",
    "Perceptron inference is much faster! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Reordering training data for positives to come first\n",
    "\n",
    "Yes, both models degraded as they don't get to correct errors until they exhaust all positive examples.\n",
    "They are now as good as just trained on negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otrain_X, otrain_y = train_X, train_y\n",
    "\n",
    "sorted_idx = np.argpartition(train_y, range(len(train_y)))[::-1]\n",
    "ordered_train_X = train_X[sorted_idx]\n",
    "ordered_train_y = np.array(train_y)[sorted_idx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 4 (0.08%) dev_error 23.60% (+:0.00%)\n",
      "epoch 2 updates 6 (0.12%) dev_error 23.60% (+:0.00%)\n",
      "epoch 3 updates 6 (0.12%) dev_error 23.60% (+:0.00%)\n",
      "epoch 4 updates 9 (0.18%) dev_error 23.60% (+:0.00%)\n",
      "epoch 5 updates 10 (0.20%) dev_error 23.60% (+:0.00%)\n",
      "epoch 6 updates 11 (0.22%) dev_error 23.60% (+:0.00%)\n",
      "epoch 7 updates 14 (0.28%) dev_error 23.60% (+:0.00%)\n",
      "epoch 8 updates 14 (0.28%) dev_error 23.60% (+:0.00%)\n",
      "epoch 9 updates 15 (0.30%) dev_error 23.60% (+:0.00%)\n",
      "epoch 10 updates 16 (0.32%) dev_error 23.60% (+:0.00%)\n",
      "epoch 11 updates 16 (0.32%) dev_error 23.60% (+:0.00%)\n",
      "epoch 12 updates 16 (0.32%) dev_error 23.60% (+:0.00%)\n",
      "epoch 13 updates 18 (0.36%) dev_error 23.60% (+:0.00%)\n",
      "epoch 14 updates 17 (0.34%) dev_error 23.60% (+:0.00%)\n",
      "epoch 15 updates 17 (0.34%) dev_error 23.60% (+:0.00%)\n",
      "epoch 16 updates 18 (0.36%) dev_error 23.60% (+:0.00%)\n",
      "epoch 17 updates 19 (0.38%) dev_error 23.60% (+:0.00%)\n",
      "epoch 18 updates 18 (0.36%) dev_error 23.60% (+:0.00%)\n",
      "epoch 19 updates 18 (0.36%) dev_error 23.60% (+:0.00%)\n",
      "epoch 20 updates 20 (0.40%) dev_error 23.60% (+:0.00%)\n",
      "epoch 21 updates 21 (0.42%) dev_error 23.60% (+:0.00%)\n",
      "epoch 22 updates 22 (0.44%) dev_error 23.60% (+:0.00%)\n",
      "epoch 23 updates 22 (0.44%) dev_error 23.60% (+:0.00%)\n",
      "epoch 24 updates 20 (0.40%) dev_error 23.60% (+:0.00%)\n",
      "epoch 25 updates 21 (0.42%) dev_error 23.60% (+:0.00%)\n",
      "epoch 26 updates 19 (0.38%) dev_error 23.60% (+:0.00%)\n",
      "epoch 27 updates 22 (0.44%) dev_error 23.60% (+:0.00%)\n",
      "epoch 28 updates 22 (0.44%) dev_error 23.60% (+:0.00%)\n",
      "epoch 29 updates 20 (0.40%) dev_error 23.60% (+:0.00%)\n",
      "epoch 30 updates 23 (0.46%) dev_error 23.60% (+:0.00%)\n",
      "epoch 31 updates 24 (0.48%) dev_error 23.60% (+:0.00%)\n",
      "epoch 32 updates 23 (0.46%) dev_error 23.60% (+:0.00%)\n",
      "epoch 33 updates 21 (0.42%) dev_error 23.60% (+:0.00%)\n",
      "epoch 34 updates 22 (0.44%) dev_error 23.50% (+:0.10%)\n",
      "epoch 35 updates 23 (0.46%) dev_error 23.60% (+:0.00%)\n",
      "epoch 36 updates 22 (0.44%) dev_error 23.50% (+:0.10%)\n",
      "epoch 37 updates 25 (0.50%) dev_error 23.60% (+:0.00%)\n",
      "epoch 38 updates 26 (0.52%) dev_error 23.60% (+:0.00%)\n",
      "epoch 39 updates 24 (0.48%) dev_error 23.60% (+:0.00%)\n",
      "epoch 40 updates 24 (0.48%) dev_error 23.60% (+:0.00%)\n",
      "epoch 41 updates 24 (0.48%) dev_error 23.60% (+:0.00%)\n",
      "epoch 42 updates 24 (0.48%) dev_error 23.60% (+:0.00%)\n",
      "epoch 43 updates 24 (0.48%) dev_error 23.60% (+:0.00%)\n",
      "epoch 44 updates 24 (0.48%) dev_error 23.60% (+:0.00%)\n",
      "epoch 45 updates 25 (0.50%) dev_error 23.60% (+:0.00%)\n",
      "epoch 46 updates 28 (0.56%) dev_error 23.60% (+:0.00%)\n",
      "epoch 47 updates 26 (0.52%) dev_error 23.60% (+:0.00%)\n",
      "epoch 48 updates 25 (0.50%) dev_error 23.60% (+:0.00%)\n",
      "epoch 49 updates 26 (0.52%) dev_error 23.50% (+:0.10%)\n",
      "epoch 50 updates 27 (0.54%) dev_error 23.50% (+:0.10%)\n"
     ]
    }
   ],
   "source": [
    "pt = Perceptron(epochs=50, n_features=ordered_train_X.shape[1])\n",
    "pt.summary(trX=ordered_train_X, trY=ordered_train_y, teX=eval_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 4 (0.08%) dev_error 23.50% (+:0.10%)\n",
      "epoch 2 updates 6 (0.12%) dev_error 23.60% (+:0.00%)\n",
      "epoch 3 updates 6 (0.12%) dev_error 23.70% (+:0.10%)\n",
      "epoch 4 updates 9 (0.18%) dev_error 23.70% (+:0.30%)\n",
      "epoch 5 updates 10 (0.20%) dev_error 23.50% (+:0.10%)\n",
      "epoch 6 updates 11 (0.22%) dev_error 23.40% (+:0.20%)\n",
      "epoch 7 updates 14 (0.28%) dev_error 23.50% (+:0.30%)\n",
      "epoch 8 updates 14 (0.28%) dev_error 23.60% (+:0.20%)\n",
      "epoch 9 updates 15 (0.30%) dev_error 23.40% (+:0.40%)\n",
      "epoch 10 updates 16 (0.32%) dev_error 23.30% (+:0.50%)\n",
      "epoch 11 updates 16 (0.32%) dev_error 23.20% (+:0.60%)\n",
      "epoch 12 updates 16 (0.32%) dev_error 23.20% (+:1.00%)\n",
      "epoch 13 updates 18 (0.36%) dev_error 23.10% (+:1.30%)\n",
      "epoch 14 updates 17 (0.34%) dev_error 23.30% (+:1.30%)\n",
      "epoch 15 updates 17 (0.34%) dev_error 23.10% (+:1.70%)\n",
      "epoch 16 updates 18 (0.36%) dev_error 23.10% (+:1.70%)\n",
      "epoch 17 updates 19 (0.38%) dev_error 22.90% (+:1.90%)\n",
      "epoch 18 updates 18 (0.36%) dev_error 22.80% (+:2.00%)\n",
      "epoch 19 updates 18 (0.36%) dev_error 22.90% (+:2.10%)\n",
      "epoch 20 updates 20 (0.40%) dev_error 22.70% (+:2.30%)\n",
      "epoch 21 updates 21 (0.42%) dev_error 22.80% (+:2.20%)\n",
      "epoch 22 updates 22 (0.44%) dev_error 22.80% (+:2.20%)\n",
      "epoch 23 updates 22 (0.44%) dev_error 22.80% (+:2.20%)\n",
      "epoch 24 updates 20 (0.40%) dev_error 22.70% (+:2.30%)\n",
      "epoch 25 updates 21 (0.42%) dev_error 22.80% (+:2.20%)\n",
      "epoch 26 updates 19 (0.38%) dev_error 22.80% (+:2.40%)\n",
      "epoch 27 updates 22 (0.44%) dev_error 22.80% (+:2.40%)\n",
      "epoch 28 updates 22 (0.44%) dev_error 22.70% (+:2.50%)\n",
      "epoch 29 updates 20 (0.40%) dev_error 22.70% (+:2.70%)\n",
      "epoch 30 updates 23 (0.46%) dev_error 22.70% (+:2.90%)\n",
      "epoch 31 updates 24 (0.48%) dev_error 22.90% (+:2.90%)\n",
      "epoch 32 updates 23 (0.46%) dev_error 22.80% (+:3.00%)\n",
      "epoch 33 updates 21 (0.42%) dev_error 22.80% (+:3.00%)\n",
      "epoch 34 updates 22 (0.44%) dev_error 22.80% (+:3.00%)\n",
      "epoch 35 updates 23 (0.46%) dev_error 22.80% (+:3.00%)\n",
      "epoch 36 updates 22 (0.44%) dev_error 22.80% (+:3.00%)\n",
      "epoch 37 updates 25 (0.50%) dev_error 22.80% (+:3.20%)\n",
      "epoch 38 updates 26 (0.52%) dev_error 22.80% (+:3.20%)\n",
      "epoch 39 updates 24 (0.48%) dev_error 22.80% (+:3.20%)\n",
      "epoch 40 updates 24 (0.48%) dev_error 23.10% (+:3.50%)\n",
      "epoch 41 updates 24 (0.48%) dev_error 23.20% (+:3.60%)\n",
      "epoch 42 updates 24 (0.48%) dev_error 23.40% (+:3.80%)\n",
      "epoch 43 updates 24 (0.48%) dev_error 23.50% (+:3.90%)\n",
      "epoch 44 updates 24 (0.48%) dev_error 23.50% (+:4.10%)\n",
      "epoch 45 updates 25 (0.50%) dev_error 23.30% (+:4.30%)\n",
      "epoch 46 updates 28 (0.56%) dev_error 23.30% (+:4.30%)\n",
      "epoch 47 updates 26 (0.52%) dev_error 23.20% (+:4.20%)\n",
      "epoch 48 updates 25 (0.50%) dev_error 23.20% (+:4.20%)\n",
      "epoch 49 updates 26 (0.52%) dev_error 23.30% (+:4.30%)\n",
      "epoch 50 updates 27 (0.54%) dev_error 23.30% (+:4.30%)\n"
     ]
    }
   ],
   "source": [
    "apt = AvgPerceptron(epochs=50, n_features=ordered_train_X.shape[1])\n",
    "apt.summary(trX=ordered_train_X, trY=ordered_train_y, teX=eval_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Appending numerical features as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_age = np.array(train_df[\"age\"]).reshape(-1, 1)\n",
    "te_age = np.array(eval_df[\"age\"]).reshape(-1, 1)\n",
    "tr_hw_per_week = np.array(train_df[\"hours-per-week\"]).astype(np.int).reshape(-1, 1)\n",
    "te_hw_per_week = np.array(eval_df[\"hours-per-week\"]).astype(np.int).reshape(-1, 1)\n",
    "\n",
    "nappended_train_X = np.hstack((train_X, tr_age, tr_hw_per_week))\n",
    "nappended_test_X = np.hstack((test_X, te_age, te_hw_per_week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1858 (37.16%) dev_error 23.80% (+:0.20%)\n",
      "epoch 2 updates 1676 (33.52%) dev_error 23.80% (+:0.20%)\n",
      "epoch 3 updates 1601 (32.02%) dev_error 36.20% (+:23.40%)\n",
      "epoch 4 updates 1516 (30.32%) dev_error 37.50% (+:25.90%)\n",
      "epoch 5 updates 1510 (30.20%) dev_error 26.50% (+:3.90%)\n"
     ]
    }
   ],
   "source": [
    "pt = Perceptron(epochs=5, n_features=nappended_train_X.shape[1])\n",
    "pt.summary(trX=nappended_train_X, trY=train_y, teX=nappended_test_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1858 (37.16%) dev_error 23.60% (+:0.00%)\n",
      "epoch 2 updates 1676 (33.52%) dev_error 23.70% (+:0.10%)\n",
      "epoch 3 updates 1601 (32.02%) dev_error 24.80% (+:1.20%)\n",
      "epoch 4 updates 1516 (30.32%) dev_error 26.00% (+:3.00%)\n",
      "epoch 5 updates 1510 (30.20%) dev_error 27.70% (+:5.70%)\n"
     ]
    }
   ],
   "source": [
    "apt = AvgPerceptron(epochs=5, n_features=nappended_train_X.shape[1])\n",
    "apt.summary(trX=nappended_train_X, trY=train_y, teX=nappended_test_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centering each numerical dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_age = np.array(train_df[\"age\"]).reshape(-1, 1)\n",
    "tr_age = tr_age - np.mean(tr_age)\n",
    "te_age = np.array(eval_df[\"age\"]).reshape(-1, 1)\n",
    "te_age = te_age - np.mean(te_age)\n",
    "\n",
    "tr_hw_per_week = np.array(train_df[\"hours-per-week\"]).astype(np.int).reshape(-1, 1)\n",
    "tr_hw_per_week = tr_hw_per_week - np.mean(tr_hw_per_week)\n",
    "te_hw_per_week = np.array(eval_df[\"hours-per-week\"]).astype(np.int).reshape(-1, 1)\n",
    "te_hw_per_week = te_hw_per_week - np.mean(te_hw_per_week)\n",
    "\n",
    "cn_nappended_train_X = np.hstack((train_X, tr_age, tr_hw_per_week))\n",
    "cn_nappended_test_X = np.hstack((test_X, te_age, te_hw_per_week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1183 (23.66%) dev_error 36.10% (+:34.50%)\n",
      "epoch 2 updates 1144 (22.88%) dev_error 29.10% (+:14.70%)\n",
      "epoch 3 updates 1139 (22.78%) dev_error 27.20% (+:14.20%)\n",
      "epoch 4 updates 1127 (22.54%) dev_error 33.70% (+:23.30%)\n",
      "epoch 5 updates 1117 (22.34%) dev_error 29.90% (+:16.70%)\n"
     ]
    }
   ],
   "source": [
    "pt = Perceptron(epochs=5, n_features=nappended_train_X.shape[1])\n",
    "pt.summary(trX=cn_nappended_train_X, trY=train_y, teX=cn_nappended_test_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1183 (23.66%) dev_error 31.10% (+:16.90%)\n",
      "epoch 2 updates 1144 (22.88%) dev_error 31.70% (+:17.70%)\n",
      "epoch 3 updates 1139 (22.78%) dev_error 31.80% (+:17.80%)\n",
      "epoch 4 updates 1127 (22.54%) dev_error 32.40% (+:18.20%)\n",
      "epoch 5 updates 1117 (22.34%) dev_error 32.60% (+:18.80%)\n"
     ]
    }
   ],
   "source": [
    "apt = AvgPerceptron(epochs=5, n_features=nappended_train_X.shape[1])\n",
    "apt.summary(trX=cn_nappended_train_X, trY=train_y, teX=cn_nappended_test_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_age = np.array(train_df[\"age\"]).reshape(-1, 1)\n",
    "tr_age = tr_age - np.mean(tr_age)\n",
    "tr_age = tr_age / np.std(tr_age)\n",
    "te_age = np.array(eval_df[\"age\"]).reshape(-1, 1)\n",
    "te_age = te_age - np.mean(te_age)\n",
    "te_age = te_age / np.std(te_age)\n",
    "\n",
    "tr_hw_per_week = np.array(train_df[\"hours-per-week\"]).astype(np.int).reshape(-1, 1)\n",
    "tr_hw_per_week = tr_hw_per_week - np.mean(tr_hw_per_week)\n",
    "tr_hw_per_week = tr_hw_per_week / np.std(tr_hw_per_week)\n",
    "te_hw_per_week = np.array(eval_df[\"hours-per-week\"]).astype(np.int).reshape(-1, 1)\n",
    "te_hw_per_week = te_hw_per_week - np.mean(te_hw_per_week)\n",
    "te_hw_per_week = te_hw_per_week / np.std(te_hw_per_week)\n",
    "\n",
    "nr_nappended_train_X = np.hstack((train_X, tr_age, tr_hw_per_week))\n",
    "nr_nappended_test_X = np.hstack((test_X, te_age, te_hw_per_week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1183 (23.66%) dev_error 36.10% (+:34.50%)\n",
      "epoch 2 updates 1144 (22.88%) dev_error 29.10% (+:14.70%)\n",
      "epoch 3 updates 1139 (22.78%) dev_error 27.20% (+:14.20%)\n",
      "epoch 4 updates 1127 (22.54%) dev_error 33.70% (+:23.30%)\n",
      "epoch 5 updates 1117 (22.34%) dev_error 29.90% (+:16.70%)\n"
     ]
    }
   ],
   "source": [
    "pt = Perceptron(epochs=5, n_features=nappended_train_X.shape[1])\n",
    "pt.summary(trX=nr_nappended_train_X, trY=train_y, teX=nr_nappended_test_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1183 (23.66%) dev_error 31.10% (+:16.90%)\n",
      "epoch 2 updates 1144 (22.88%) dev_error 31.70% (+:17.70%)\n",
      "epoch 3 updates 1139 (22.78%) dev_error 31.80% (+:17.80%)\n",
      "epoch 4 updates 1127 (22.54%) dev_error 32.40% (+:18.20%)\n",
      "epoch 5 updates 1117 (22.34%) dev_error 32.60% (+:18.80%)\n"
     ]
    }
   ],
   "source": [
    "apt = AvgPerceptron(epochs=5, n_features=nappended_train_X.shape[1])\n",
    "apt.summary(trX=nr_nappended_train_X, trY=train_y, teX=nr_nappended_test_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding some binary combination features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_combinations(fa, fb, df):\n",
    "    fa_enco = one_hot_encoders[fa]\n",
    "    fb_enco = one_hot_encoders[fb]\n",
    "    fa_catg = fa_enco.catg_index\n",
    "    fb_catg = fb_enco.catg_index\n",
    "    n_fa_catg = len(fa_catg)\n",
    "    n_fb_catg = len(fb_catg)\n",
    "    \n",
    "\n",
    "    enum = 0\n",
    "    fa_vals, fb_vals = df[fa], df[fb]\n",
    "    combinations = np.zeros((len(fa_vals), n_fa_catg * n_fb_catg))\n",
    "    \n",
    "    error_count = 0\n",
    "    for fai, fbi in zip(*[fa_vals, fb_vals]):\n",
    "        try:\n",
    "            aidx = fa_catg[fai]\n",
    "            bidx = fb_catg[fbi]\n",
    "        except KeyError:\n",
    "            error_count += 1\n",
    "        \n",
    "        linear_idx = n_fb_catg * aidx + bidx\n",
    "        combinations[enum, linear_idx] = 1\n",
    "        enum += 1\n",
    "    \n",
    "    print(f'Failed computing binary combinations for {error_count} examples.')\n",
    "\n",
    "    return combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For feature combo: age + sector\n",
      "epoch 1 updates 1256 (25.12%) dev_error 16.10% (+:19.50%)\n",
      "epoch 2 updates 1193 (23.86%) dev_error 15.00% (+:20.00%)\n",
      "epoch 3 updates 1141 (22.82%) dev_error 15.10% (+:20.50%)\n",
      "epoch 4 updates 1154 (23.08%) dev_error 15.50% (+:20.30%)\n",
      "epoch 5 updates 1118 (22.36%) dev_error 15.80% (+:20.60%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: age + education\n",
      "epoch 1 updates 1253 (25.06%) dev_error 15.30% (+:18.70%)\n",
      "epoch 2 updates 1167 (23.34%) dev_error 15.30% (+:19.50%)\n",
      "epoch 3 updates 1120 (22.40%) dev_error 15.90% (+:20.30%)\n",
      "epoch 4 updates 1138 (22.76%) dev_error 15.80% (+:20.60%)\n",
      "epoch 5 updates 1090 (21.80%) dev_error 15.50% (+:20.70%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: age + marital-status\n",
      "epoch 1 updates 1228 (24.56%) dev_error 14.80% (+:19.00%)\n",
      "epoch 2 updates 1178 (23.56%) dev_error 15.30% (+:19.90%)\n",
      "epoch 3 updates 1170 (23.40%) dev_error 15.20% (+:19.80%)\n",
      "epoch 4 updates 1180 (23.60%) dev_error 15.40% (+:19.80%)\n",
      "epoch 5 updates 1167 (23.34%) dev_error 15.80% (+:20.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: age + occupation\n",
      "epoch 1 updates 1252 (25.04%) dev_error 15.30% (+:19.30%)\n",
      "epoch 2 updates 1162 (23.24%) dev_error 15.40% (+:20.00%)\n",
      "epoch 3 updates 1149 (22.98%) dev_error 16.10% (+:20.70%)\n",
      "epoch 4 updates 1086 (21.72%) dev_error 16.70% (+:20.90%)\n",
      "epoch 5 updates 1095 (21.90%) dev_error 16.90% (+:20.90%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: age + race\n",
      "epoch 1 updates 1256 (25.12%) dev_error 14.90% (+:19.90%)\n",
      "epoch 2 updates 1208 (24.16%) dev_error 15.20% (+:20.20%)\n",
      "epoch 3 updates 1169 (23.38%) dev_error 15.30% (+:20.10%)\n",
      "epoch 4 updates 1160 (23.20%) dev_error 15.30% (+:20.70%)\n",
      "epoch 5 updates 1156 (23.12%) dev_error 15.50% (+:20.50%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: age + gender\n",
      "epoch 1 updates 1226 (24.52%) dev_error 15.60% (+:20.60%)\n",
      "epoch 2 updates 1178 (23.56%) dev_error 15.20% (+:20.20%)\n",
      "epoch 3 updates 1171 (23.42%) dev_error 15.50% (+:20.30%)\n",
      "epoch 4 updates 1145 (22.90%) dev_error 15.30% (+:20.90%)\n",
      "epoch 5 updates 1188 (23.76%) dev_error 15.60% (+:21.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: age + hours-per-week\n",
      "epoch 1 updates 1260 (25.20%) dev_error 15.50% (+:18.50%)\n",
      "epoch 2 updates 1165 (23.30%) dev_error 15.20% (+:19.80%)\n",
      "epoch 3 updates 1160 (23.20%) dev_error 15.30% (+:20.10%)\n",
      "epoch 4 updates 1122 (22.44%) dev_error 15.80% (+:20.20%)\n",
      "epoch 5 updates 1090 (21.80%) dev_error 16.60% (+:19.80%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: age + country-of-origin\n",
      "epoch 1 updates 1268 (25.36%) dev_error 15.10% (+:19.90%)\n",
      "epoch 2 updates 1191 (23.82%) dev_error 14.80% (+:19.60%)\n",
      "epoch 3 updates 1163 (23.26%) dev_error 15.10% (+:19.70%)\n",
      "epoch 4 updates 1176 (23.52%) dev_error 14.60% (+:19.60%)\n",
      "epoch 5 updates 1163 (23.26%) dev_error 14.80% (+:20.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: sector + education\n",
      "epoch 1 updates 1234 (24.68%) dev_error 14.90% (+:18.90%)\n",
      "epoch 2 updates 1201 (24.02%) dev_error 15.00% (+:19.60%)\n",
      "epoch 3 updates 1206 (24.12%) dev_error 15.70% (+:19.70%)\n",
      "epoch 4 updates 1147 (22.94%) dev_error 16.10% (+:19.70%)\n",
      "epoch 5 updates 1156 (23.12%) dev_error 15.80% (+:20.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: sector + marital-status\n",
      "epoch 1 updates 1256 (25.12%) dev_error 15.00% (+:19.00%)\n",
      "epoch 2 updates 1168 (23.36%) dev_error 14.60% (+:19.60%)\n",
      "epoch 3 updates 1171 (23.42%) dev_error 14.80% (+:19.60%)\n",
      "epoch 4 updates 1155 (23.10%) dev_error 14.70% (+:19.90%)\n",
      "epoch 5 updates 1121 (22.42%) dev_error 15.10% (+:20.10%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: sector + occupation\n",
      "epoch 1 updates 1235 (24.70%) dev_error 15.70% (+:19.30%)\n",
      "epoch 2 updates 1203 (24.06%) dev_error 15.40% (+:20.00%)\n",
      "epoch 3 updates 1185 (23.70%) dev_error 15.60% (+:20.00%)\n",
      "epoch 4 updates 1187 (23.74%) dev_error 15.50% (+:20.30%)\n",
      "epoch 5 updates 1161 (23.22%) dev_error 15.90% (+:20.30%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: sector + race\n",
      "epoch 1 updates 1235 (24.70%) dev_error 15.00% (+:18.40%)\n",
      "epoch 2 updates 1175 (23.50%) dev_error 14.90% (+:18.90%)\n",
      "epoch 3 updates 1156 (23.12%) dev_error 15.00% (+:19.20%)\n",
      "epoch 4 updates 1193 (23.86%) dev_error 15.00% (+:20.20%)\n",
      "epoch 5 updates 1150 (23.00%) dev_error 15.10% (+:19.90%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: sector + gender\n",
      "epoch 1 updates 1261 (25.22%) dev_error 14.90% (+:19.10%)\n",
      "epoch 2 updates 1173 (23.46%) dev_error 15.10% (+:19.30%)\n",
      "epoch 3 updates 1156 (23.12%) dev_error 14.90% (+:19.70%)\n",
      "epoch 4 updates 1157 (23.14%) dev_error 15.20% (+:19.80%)\n",
      "epoch 5 updates 1179 (23.58%) dev_error 14.90% (+:19.90%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: sector + hours-per-week\n",
      "epoch 1 updates 1251 (25.02%) dev_error 15.50% (+:19.10%)\n",
      "epoch 2 updates 1181 (23.62%) dev_error 15.70% (+:19.50%)\n",
      "epoch 3 updates 1164 (23.28%) dev_error 15.60% (+:19.60%)\n",
      "epoch 4 updates 1155 (23.10%) dev_error 15.30% (+:19.50%)\n",
      "epoch 5 updates 1162 (23.24%) dev_error 15.50% (+:19.90%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: sector + country-of-origin\n",
      "epoch 1 updates 1237 (24.74%) dev_error 15.20% (+:19.00%)\n",
      "epoch 2 updates 1189 (23.78%) dev_error 14.80% (+:19.20%)\n",
      "epoch 3 updates 1176 (23.52%) dev_error 14.90% (+:19.90%)\n",
      "epoch 4 updates 1166 (23.32%) dev_error 14.90% (+:20.50%)\n",
      "epoch 5 updates 1165 (23.30%) dev_error 14.80% (+:20.20%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: education + marital-status\n",
      "epoch 1 updates 1242 (24.84%) dev_error 15.40% (+:20.00%)\n",
      "epoch 2 updates 1187 (23.74%) dev_error 15.60% (+:20.00%)\n",
      "epoch 3 updates 1178 (23.56%) dev_error 15.20% (+:20.40%)\n",
      "epoch 4 updates 1143 (22.86%) dev_error 15.30% (+:20.50%)\n",
      "epoch 5 updates 1148 (22.96%) dev_error 15.60% (+:20.60%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: education + occupation\n",
      "epoch 1 updates 1256 (25.12%) dev_error 15.50% (+:18.30%)\n",
      "epoch 2 updates 1218 (24.36%) dev_error 15.10% (+:19.50%)\n",
      "epoch 3 updates 1148 (22.96%) dev_error 15.10% (+:19.50%)\n",
      "epoch 4 updates 1155 (23.10%) dev_error 15.50% (+:19.70%)\n",
      "epoch 5 updates 1157 (23.14%) dev_error 15.60% (+:20.20%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: education + race\n",
      "epoch 1 updates 1236 (24.72%) dev_error 15.70% (+:19.50%)\n",
      "epoch 2 updates 1190 (23.80%) dev_error 15.40% (+:19.80%)\n",
      "epoch 3 updates 1184 (23.68%) dev_error 15.70% (+:20.30%)\n",
      "epoch 4 updates 1180 (23.60%) dev_error 15.70% (+:20.30%)\n",
      "epoch 5 updates 1144 (22.88%) dev_error 15.60% (+:20.40%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: education + gender\n",
      "epoch 1 updates 1255 (25.10%) dev_error 14.70% (+:18.70%)\n",
      "epoch 2 updates 1165 (23.30%) dev_error 14.40% (+:19.20%)\n",
      "epoch 3 updates 1179 (23.58%) dev_error 14.90% (+:19.70%)\n",
      "epoch 4 updates 1164 (23.28%) dev_error 14.80% (+:20.20%)\n",
      "epoch 5 updates 1150 (23.00%) dev_error 15.20% (+:20.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: education + hours-per-week\n",
      "epoch 1 updates 1264 (25.28%) dev_error 14.60% (+:18.20%)\n",
      "epoch 2 updates 1188 (23.76%) dev_error 15.30% (+:19.30%)\n",
      "epoch 3 updates 1165 (23.30%) dev_error 15.00% (+:19.20%)\n",
      "epoch 4 updates 1143 (22.86%) dev_error 15.30% (+:20.10%)\n",
      "epoch 5 updates 1145 (22.90%) dev_error 15.40% (+:20.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: education + country-of-origin\n",
      "epoch 1 updates 1271 (25.42%) dev_error 14.50% (+:18.30%)\n",
      "epoch 2 updates 1173 (23.46%) dev_error 14.90% (+:20.50%)\n",
      "epoch 3 updates 1194 (23.88%) dev_error 15.10% (+:20.30%)\n",
      "epoch 4 updates 1187 (23.74%) dev_error 15.10% (+:20.30%)\n",
      "epoch 5 updates 1147 (22.94%) dev_error 15.70% (+:20.70%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: marital-status + occupation\n",
      "epoch 1 updates 1254 (25.08%) dev_error 15.30% (+:19.10%)\n",
      "epoch 2 updates 1184 (23.68%) dev_error 14.80% (+:20.20%)\n",
      "epoch 3 updates 1189 (23.78%) dev_error 14.70% (+:20.30%)\n",
      "epoch 4 updates 1182 (23.64%) dev_error 14.90% (+:20.70%)\n",
      "epoch 5 updates 1188 (23.76%) dev_error 14.90% (+:20.50%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: marital-status + race\n",
      "epoch 1 updates 1253 (25.06%) dev_error 15.40% (+:19.80%)\n",
      "epoch 2 updates 1209 (24.18%) dev_error 14.80% (+:20.20%)\n",
      "epoch 3 updates 1195 (23.90%) dev_error 15.00% (+:21.00%)\n",
      "epoch 4 updates 1200 (24.00%) dev_error 15.00% (+:21.00%)\n",
      "epoch 5 updates 1176 (23.52%) dev_error 15.30% (+:21.10%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: marital-status + gender\n",
      "epoch 1 updates 1226 (24.52%) dev_error 14.60% (+:18.80%)\n",
      "epoch 2 updates 1186 (23.72%) dev_error 15.10% (+:20.10%)\n",
      "epoch 3 updates 1177 (23.54%) dev_error 14.40% (+:20.00%)\n",
      "epoch 4 updates 1184 (23.68%) dev_error 14.80% (+:20.60%)\n",
      "epoch 5 updates 1160 (23.20%) dev_error 15.00% (+:20.60%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: marital-status + hours-per-week\n",
      "epoch 1 updates 1251 (25.02%) dev_error 14.10% (+:19.10%)\n",
      "epoch 2 updates 1208 (24.16%) dev_error 14.20% (+:19.60%)\n",
      "epoch 3 updates 1173 (23.46%) dev_error 14.20% (+:19.60%)\n",
      "epoch 4 updates 1150 (23.00%) dev_error 14.60% (+:19.60%)\n",
      "epoch 5 updates 1159 (23.18%) dev_error 15.00% (+:20.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: marital-status + country-of-origin\n",
      "epoch 1 updates 1258 (25.16%) dev_error 15.20% (+:19.00%)\n",
      "epoch 2 updates 1192 (23.84%) dev_error 15.00% (+:19.20%)\n",
      "epoch 3 updates 1182 (23.64%) dev_error 14.90% (+:19.70%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 updates 1194 (23.88%) dev_error 15.00% (+:20.00%)\n",
      "epoch 5 updates 1148 (22.96%) dev_error 15.00% (+:20.40%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: occupation + race\n",
      "epoch 1 updates 1262 (25.24%) dev_error 14.90% (+:18.10%)\n",
      "epoch 2 updates 1199 (23.98%) dev_error 15.00% (+:20.00%)\n",
      "epoch 3 updates 1178 (23.56%) dev_error 15.40% (+:20.60%)\n",
      "epoch 4 updates 1172 (23.44%) dev_error 15.60% (+:20.20%)\n",
      "epoch 5 updates 1169 (23.38%) dev_error 15.60% (+:20.60%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: occupation + gender\n",
      "epoch 1 updates 1254 (25.08%) dev_error 14.10% (+:18.90%)\n",
      "epoch 2 updates 1201 (24.02%) dev_error 14.60% (+:19.80%)\n",
      "epoch 3 updates 1192 (23.84%) dev_error 14.40% (+:20.60%)\n",
      "epoch 4 updates 1141 (22.82%) dev_error 14.50% (+:20.30%)\n",
      "epoch 5 updates 1199 (23.98%) dev_error 14.60% (+:20.40%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: occupation + hours-per-week\n",
      "epoch 1 updates 1248 (24.96%) dev_error 14.90% (+:20.70%)\n",
      "epoch 2 updates 1186 (23.72%) dev_error 15.00% (+:21.00%)\n",
      "epoch 3 updates 1152 (23.04%) dev_error 15.20% (+:20.60%)\n",
      "epoch 4 updates 1152 (23.04%) dev_error 15.50% (+:20.70%)\n",
      "epoch 5 updates 1128 (22.56%) dev_error 15.60% (+:21.00%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: occupation + country-of-origin\n",
      "epoch 1 updates 1272 (25.44%) dev_error 14.80% (+:18.60%)\n",
      "epoch 2 updates 1208 (24.16%) dev_error 14.00% (+:19.80%)\n",
      "epoch 3 updates 1176 (23.52%) dev_error 14.20% (+:19.80%)\n",
      "epoch 4 updates 1163 (23.26%) dev_error 14.70% (+:20.30%)\n",
      "epoch 5 updates 1154 (23.08%) dev_error 15.10% (+:20.70%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: race + gender\n",
      "epoch 1 updates 1277 (25.54%) dev_error 14.30% (+:18.70%)\n",
      "epoch 2 updates 1203 (24.06%) dev_error 14.60% (+:19.40%)\n",
      "epoch 3 updates 1189 (23.78%) dev_error 15.10% (+:20.10%)\n",
      "epoch 4 updates 1181 (23.62%) dev_error 15.00% (+:20.40%)\n",
      "epoch 5 updates 1170 (23.40%) dev_error 15.00% (+:20.60%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: race + hours-per-week\n",
      "epoch 1 updates 1268 (25.36%) dev_error 15.40% (+:18.40%)\n",
      "epoch 2 updates 1190 (23.80%) dev_error 15.10% (+:19.70%)\n",
      "epoch 3 updates 1146 (22.92%) dev_error 15.50% (+:19.90%)\n",
      "epoch 4 updates 1162 (23.24%) dev_error 15.50% (+:20.30%)\n",
      "epoch 5 updates 1155 (23.10%) dev_error 14.80% (+:20.20%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: race + country-of-origin\n",
      "epoch 1 updates 1247 (24.94%) dev_error 15.00% (+:19.00%)\n",
      "epoch 2 updates 1182 (23.64%) dev_error 14.40% (+:19.60%)\n",
      "epoch 3 updates 1161 (23.22%) dev_error 14.90% (+:20.50%)\n",
      "epoch 4 updates 1191 (23.82%) dev_error 14.80% (+:20.40%)\n",
      "epoch 5 updates 1166 (23.32%) dev_error 15.30% (+:20.50%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: gender + hours-per-week\n",
      "epoch 1 updates 1263 (25.26%) dev_error 15.30% (+:18.90%)\n",
      "epoch 2 updates 1179 (23.58%) dev_error 14.60% (+:19.80%)\n",
      "epoch 3 updates 1161 (23.22%) dev_error 14.80% (+:20.20%)\n",
      "epoch 4 updates 1174 (23.48%) dev_error 14.80% (+:20.60%)\n",
      "epoch 5 updates 1157 (23.14%) dev_error 15.20% (+:20.20%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: gender + country-of-origin\n",
      "epoch 1 updates 1252 (25.04%) dev_error 15.40% (+:19.40%)\n",
      "epoch 2 updates 1191 (23.82%) dev_error 15.10% (+:20.10%)\n",
      "epoch 3 updates 1187 (23.74%) dev_error 15.00% (+:20.60%)\n",
      "epoch 4 updates 1186 (23.72%) dev_error 15.30% (+:20.50%)\n",
      "epoch 5 updates 1165 (23.30%) dev_error 15.40% (+:20.60%)\n",
      "\n",
      "\n",
      "\n",
      "For feature combo: hours-per-week + country-of-origin\n",
      "epoch 1 updates 1243 (24.86%) dev_error 15.90% (+:18.90%)\n",
      "epoch 2 updates 1195 (23.90%) dev_error 15.50% (+:19.90%)\n",
      "epoch 3 updates 1185 (23.70%) dev_error 14.90% (+:19.30%)\n",
      "epoch 4 updates 1165 (23.30%) dev_error 14.80% (+:19.80%)\n",
      "epoch 5 updates 1162 (23.24%) dev_error 14.90% (+:19.50%)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "BIN_COMBOS = [\n",
    "    \"age\",\n",
    "    \"sector\",\n",
    "    \"education\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"hours-per-week\",\n",
    "    \"country-of-origin\",\n",
    "#     \"target\"\n",
    "]\n",
    "\n",
    "for fa, fb in combinations(BIN_COMBOS, 2):\n",
    "    print(f'For feature combo: {fa} + {fb}')\n",
    "    extra_features = make_binary_combinations(fa, fb, df=train_df)\n",
    "    unified_train_X = np.hstack((train_X, extra_features))\n",
    "    \n",
    "    extra_features = make_binary_combinations(fa, fb, df=eval_df)\n",
    "    unified_test_X = np.hstack((eval_X, extra_features))\n",
    "    \n",
    "    apt = AvgPerceptron(epochs=5, n_features=unified_train_X.shape[1])\n",
    "    apt.summary(trX=unified_train_X, trY=train_y, teX=unified_test_X, teY=eval_y)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1237 (24.74%) dev_error 14.60% (+:20.40%)\n",
      "epoch 2 updates 1152 (23.04%) dev_error 14.60% (+:20.60%)\n",
      "epoch 3 updates 1167 (23.34%) dev_error 14.60% (+:21.20%)\n",
      "epoch 4 updates 1136 (22.72%) dev_error 14.80% (+:21.20%)\n",
      "epoch 5 updates 1129 (22.58%) dev_error 15.30% (+:21.90%)\n"
     ]
    }
   ],
   "source": [
    "edu_marr_vector = make_binary_combinations(\"education\", \"marital-status\", df=train_df)\n",
    "occ_gend_vector = make_binary_combinations(\"occupation\", \"gender\", df=train_df)\n",
    "marr_hours_vector = make_binary_combinations(\"marital-status\", \"hours-per-week\", df=train_df)\n",
    "unified_train_X = np.hstack((train_X, edu_marr_vector, occ_gend_vector, marr_hours_vector))\n",
    "\n",
    "edu_marr_vector = make_binary_combinations(\"education\", \"marital-status\", df=eval_df)\n",
    "occ_gend_vector = make_binary_combinations(\"occupation\", \"gender\", df=eval_df)\n",
    "marr_hours_vector = make_binary_combinations(\"marital-status\", \"hours-per-week\", df=eval_df)\n",
    "unified_test_X = np.hstack((eval_X, edu_marr_vector, occ_gend_vector, marr_hours_vector))\n",
    "\n",
    "apt = AvgPerceptron(epochs=5, n_features=unified_train_X.shape[1])\n",
    "apt.summary(trX=unified_train_X, trY=train_y, teX=unified_test_X, teY=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marital status and hours per week combination gave best error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For feature combo: marital-status + hours-per-week\n",
      "epoch 1 updates 1251 (25.02%) dev_error 14.10% (+:19.10%)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fa = \"marital-status\"\n",
    "fb = \"hours-per-week\"\n",
    "print(f'For feature combo: {fa} + {fb}')\n",
    "extra_features = make_binary_combinations(fa, fb, df=train_df)\n",
    "unified_train_X = np.hstack((train_X, extra_features))\n",
    "\n",
    "extra_features = make_binary_combinations(fa, fb, df=eval_df)\n",
    "unified_test_X = np.hstack((eval_X, extra_features))\n",
    "\n",
    "apt = AvgPerceptron(epochs=1, n_features=unified_train_X.shape[1])\n",
    "apt.summary(trX=unified_train_X, trY=train_y, teX=unified_test_X, teY=eval_y)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed computing binary combinations for 2 examples.\n",
      "Positive rate on test set: 19.8\n"
     ]
    }
   ],
   "source": [
    "extra_features = make_binary_combinations(fa, fb, df=test_df)\n",
    "unified_test_X = np.hstack((np.ones((len(test_X), 1)), test_X, extra_features))\n",
    "\n",
    "effective_W = apt.c * apt.W - apt.av_W\n",
    "predictions = (np.dot(unified_test_X, effective_W) > 0)\n",
    "# predictions[predictions == True] = \">50K\"\n",
    "# predictions[predictions == False] = \"<=50K\"\n",
    "\n",
    "predictions_labels = []\n",
    "for pred in predictions:\n",
    "    if pred == True:\n",
    "        predictions_labels.append(\">50K\")\n",
    "    else:\n",
    "        predictions_labels.append(\"<=50K\")\n",
    "\n",
    "with open(\"predictions.y.out\", \"w+\") as out:\n",
    "    out.write(\"\\n\".join(predictions_labels))\n",
    "    \n",
    "print(f'Positive rate on test set: {np.mean(predictions) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Best error rate on dev is `14.10%`. Achieved when a couple of binary features are combinedly added.\n",
    "* Postive rate is `19.10%` compared to nearly `25%` on train set.\n",
    "* Positive rate on test set: `19.80%`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debriefing \n",
    "\n",
    "1. Approximately how many hours did you spend on this assignment?\n",
    "   - 7 hours\n",
    "2. Would you rate it as easy, moderate, or difficult?\n",
    "   - Moderate\n",
    "3. Did you work on it mostly alone, or mostly with other people?\n",
    "   - Mostly alone\n",
    "4. How deeply do you feel you understand the material it covers (0%–100%)?\n",
    "   - 80%\n",
    "5. Any other comments?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit2e16e028ce8b4007b970cece9146d16d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
