{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "### What are the positive % of training data? What about the dev set? Does it make sense given your knowledge of the average per capita income in the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3749  <=50K\r\n",
      "1251  >50K\r\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "# Target categories and their counts\n",
    "!cat hw1-data/income.train.txt.5k | cut -f 10 -d \",\" | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%>50K: 25.019999999999996%\n"
     ]
    }
   ],
   "source": [
    "print(f'%>50K: {(1251/5000)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 764  <=50K\r\n",
      " 236  >50K\r\n"
     ]
    }
   ],
   "source": [
    "# Dev data\n",
    "# Target categories and their counts\n",
    "!cat hw1-data/income.dev.txt | cut -f 10 -d \",\" | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%>50K: 23.599999999999998%\n"
     ]
    }
   ],
   "source": [
    "print(f'%>50K: {(236/1000)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the youngest and oldest ages in the training set? What are the least and most amounts of hours per week do people in this set work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Youngest: 17\n",
      "Oldest: 90\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Youngest and oldeest ages in training set\n",
    "echo \"Youngest: $(cat hw1-data/income.train.txt.5k | cut -f 1 -d \",\" | sort | head -1)\"  \n",
    "echo \"Oldest: $(cat hw1-data/income.train.txt.5k | cut -f 1 -d \",\" | sort | tail -1)\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least num of working hours:  1\n",
      "Highest num of working hours:  99\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Least and most amount of working hours in training set\n",
    "echo \"Least num of working hours: $(cat hw1-data/income.train.txt.5k | cut -f 8 -d \",\" | sort | head -1)\"  \n",
    "echo \"Highest num of working hours: $(cat hw1-data/income.train.txt.5k | cut -f 8 -d \",\" | sort | tail -1)\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need to binarize all categorical fields?\n",
    "\n",
    "<br>\n",
    "Although there are some algorithms that could work on label data directly, most require the input data to be in numerical form. Meaning, as vectors in N dimensional feature space – so that they can apply vector ops as per the specified optimization algorithm to achieve an objective (like learning a pattern by minimizing a metric).\n",
    "\n",
    "Okay, but can't we directly map categorical variables to numeric labels and represent them as vectors without binarizing them? Yes, but if the categorical variable is not an ordinal (but nominal), we would be introducing a bias by labeling some classes with bigger numbers and the others with smaller. How is that problematic? **We expect nominal classes will all have equal weightage (equidistant in ND space)** but they won't be. For example, if we map `[\"White\", \"Black\", \"Asian\"]` to `[1, 2, 3]` repectively; each category (vector 1, 2, 3) has a different distance (1, 2, & 3 respectively). To nullify this bias, we use one hot encoding: `[\"White\", \"Black\", \"Asian\"]` to `[[1, 0, 0], [0, 1, 0], [0, 0, 1]]` repectively. If you calculate distances now, they all are equidistant (as it should be). \n",
    "\n",
    "Binarizing categorical fields enables us to represent them **independently** in N dimensional space.\n",
    "After one hot encoding, each possible category will have its own dimension and will not spill into other values with-in the same category OR other categories. \n",
    "\n",
    "This sort of representation will help modelling algorithms see the data in purest form without any induced language related bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we do not count age and hours, what’s maximum possible Euclidean and Manhattan distances between two training examples? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coln:(1) ->       67\n",
      "coln:(2) ->        7\n",
      "coln:(3) ->       16\n",
      "coln:(4) ->        7\n",
      "coln:(5) ->       14\n",
      "coln:(6) ->        5\n",
      "coln:(7) ->        2\n",
      "coln:(8) ->       73\n",
      "coln:(9) ->       39\n",
      "coln:(10) ->        2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Num of different categorical variables for each column:\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"coln:($i) -> $(cat hw1-data/income.train.txt.5k| cut -f $i -d \",\"| sort | uniq | wc -l)\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of all possible categories: `7 + 16 + 7 + 14 + 5 + 2 + 39 + 2 = 92`. \n",
    "So, each instance in training set is a vector of length 92 (when age and hours are omitted).\n",
    "\n",
    "Theoritically, if X, Y are two training examples, the both distances will be maximized when `X = [1, 1, .... 1]; Y = [0, 0, .... 0]`.\n",
    "\n",
    "In such case, both Euclidean distance is bounded by $d_{eu} = \\sqrt{92}$ and Manhattan distances is bounded by $d_{mn} = 92$ \n",
    "\n",
    "This is because, each term in either formula (${x_i}^2 - {y_i}^2$ or $|x_i - y_i|$) are bounded by $1$ and there are 92 such terms, one for each possible category across all columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we do not want to binarize the two numerical fields, age and hours? What if we did? How should we define the distances on these two dimensions so that each field has equal weight? (In other words, the distance induced by each field should be bounded by 2 (N.B.: not 1! why?)).\n",
    "\n",
    "[NO CLUE!]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features do you have in total (i.e., the dimensionality)? Hint: should be around 100 90. How many features do you allocate for each of the 9 fields?\n",
    "\n",
    "\n",
    "<br>\n",
    "Keep age and hours continuos, we have 92.\n",
    "\n",
    "Distribution:\n",
    "```\n",
    "coln:(1) ->        1\n",
    "coln:(2) ->        7\n",
    "coln:(3) ->       16\n",
    "coln:(4) ->        7\n",
    "coln:(5) ->       14\n",
    "coln:(6) ->        5\n",
    "coln:(7) ->        2\n",
    "coln:(8) ->        1\n",
    "coln:(9) ->       39\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How many features would you have in total if you binarize all fields?\n",
    "\n",
    "$90 + 67 + 73 = 230$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Calculating Manhattan and Euclidean Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"./hw1-data\")\n",
    "\n",
    "train_path = DATA_DIR / \"income.train.txt.5k\"\n",
    "eval_path = DATA_DIR / \"income.dev.txt\"\n",
    "test_path = DATA_DIR / \"income.test.blind\"\n",
    "train_and_eval_path = DATA_DIR / \"income.combined.6k\"\n",
    "\n",
    "COL_NAMES = [\n",
    "    \"age\",\n",
    "    \"sector\",\n",
    "    \"education\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"hours-per-week\",\n",
    "    \"country-of-origin\",\n",
    "    \"target\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_to_df(file_path):\n",
    "        \n",
    "    DELIMITER = \",\"\n",
    "    \n",
    "    def parse_row(row, delimiter=DELIMITER):\n",
    "        cells = row.split(delimiter)\n",
    "        parsed_row = [\n",
    "            int(cell.strip()) \n",
    "            if cell.isnumeric() \n",
    "            else cell.strip()\n",
    "            for cell in cells \n",
    "        ]\n",
    "        \n",
    "        return parsed_row\n",
    "        \n",
    "    data = []\n",
    "    with open(file_path) as in_:\n",
    "        raw_rows = in_.readlines()\n",
    "        \n",
    "    for row in raw_rows:\n",
    "        parsed_row = parse_row(row)\n",
    "        data.append(parsed_row)\n",
    "    \n",
    "    df = {col: val for col, val in zip(*[COL_NAMES, zip(*data)])}\n",
    "    return df, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50,\n",
       "  'Self-emp-not-inc',\n",
       "  'Bachelors',\n",
       "  'Married-civ-spouse',\n",
       "  'Exec-managerial',\n",
       "  'White',\n",
       "  'Male',\n",
       "  '13',\n",
       "  'United-States',\n",
       "  '<=50K'],\n",
       " dict_keys(['age', 'sector', 'education', 'marital-status', 'occupation', 'race', 'gender', 'hours-per-week', 'country-of-origin', 'target']))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First person in eval\n",
    "df, data = txt_file_to_df(train_path)\n",
    "data[0], df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder\n",
    "\n",
    "class OneHotEncoder(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, column):\n",
    "        self.unique_categories = list(set(column))\n",
    "        self.catg_index = {catg: cid for cid, catg in enumerate(self.unique_categories)}\n",
    "        \n",
    "    def transform(self, column):\n",
    "        ct, ck = 0, []\n",
    "        self.ohe_column = np.zeros((len(column), len(self.unique_categories)), dtype=np.float64)\n",
    "        for i, catg in enumerate(column):\n",
    "            try:\n",
    "                j = self.catg_index[catg]\n",
    "                self.ohe_column[i, j] = 1\n",
    "            except KeyError:\n",
    "                ct += 1\n",
    "                ck.append(catg)\n",
    "                \n",
    "        print(f'Failed {ct} times because of {ck}')\n",
    "        return self.ohe_column\n",
    "    \n",
    "    def fit_transform(self, column):\n",
    "        self.fit(column=column)\n",
    "        \n",
    "        return self.transform(column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sector': <__main__.OneHotEncoder at 0x112bfedd0>,\n",
       " 'education': <__main__.OneHotEncoder at 0x112bfe4d0>,\n",
       " 'marital-status': <__main__.OneHotEncoder at 0x112bfee50>,\n",
       " 'occupation': <__main__.OneHotEncoder at 0x11657d390>,\n",
       " 'race': <__main__.OneHotEncoder at 0x11657d310>,\n",
       " 'gender': <__main__.OneHotEncoder at 0x11657d4d0>,\n",
       " 'country-of-origin': <__main__.OneHotEncoder at 0x11657dad0>}"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding categorical vars\n",
    "\n",
    "def make_encoders(categorical_columns, train_df):\n",
    "    '''\n",
    "    Takes categorical column names and df to make encoder objects using data\n",
    "    '''\n",
    "\n",
    "    catg_columns = categorical_columns.keys()\n",
    "\n",
    "    # Init encoder objects\n",
    "    col_encoders = {col: OneHotEncoder() for col in catg_columns}\n",
    "    \n",
    "    # Train encoders\n",
    "    for col in catg_columns:\n",
    "        col_values = col_encoders[col].fit(train_df[col]) \n",
    "        \n",
    "    return col_encoders\n",
    "\n",
    "\n",
    "cols_catg = {\n",
    "    \"sector\": 7,\n",
    "    \"education\": 16,\n",
    "    \"marital-status\": 4,\n",
    "    \"occupation\": 14,\n",
    "    \"race\": 5,\n",
    "    \"gender\": 2,\n",
    "    \"country-of-origin\": 39,\n",
    "}\n",
    "\n",
    "combined_df, _ = txt_file_to_df(train_and_eval_path)\n",
    "\n",
    "make_encoders(categorical_columns=cols_catg, train_df=combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Takes file name and trained encoders as input to \n",
    "load, parse and transform dataset into modelling ready dataframe\n",
    "'''\n",
    "\n",
    "def txt_to_encoded_df(file_path, encoders):\n",
    "    \n",
    "    # Load and parse data\n",
    "    df, _ = txt_file_to_df(file_path)\n",
    "    \n",
    "    # Encode and transform each col\n",
    "    encoded_df = []\n",
    "    for col in df.keys():\n",
    "\n",
    "        if col == \"target\":\n",
    "            continue\n",
    "\n",
    "        elif col in encoders:\n",
    "            encoder = encoders[col]\n",
    "            col_values = encoder.transform(df[col]) \n",
    "\n",
    "        else:\n",
    "            col_values = np.array(df[col], dtype=np.float64).reshape(len(df[col]), -1) / 50.\n",
    "\n",
    "        encoded_df.append(col_values)\n",
    "\n",
    "    # Make a flat dataset from all cols\n",
    "    encoded_df = np.hstack(encoded_df)\n",
    "    \n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "(5000, 93)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.  , 1.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.76, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [1.06, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [1.22, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.84, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.42, 0.  , 0.  , ..., 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of this dataset's ETL cycle\n",
    "\n",
    "combined_df_raw, _ = txt_file_to_df(train_and_eval_path)\n",
    "one_hot_encoders = make_encoders(categorical_columns=cols_catg, train_df=combined_df_raw)\n",
    "train_df = txt_to_encoded_df(file_path=train_path, encoders=one_hot_encoders)\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n",
      "Failed 0 times because of []\n"
     ]
    }
   ],
   "source": [
    "train_df = txt_to_encoded_df(file_path=train_path, encoders=one_hot_encoders)\n",
    "eval_df = txt_to_encoded_df(file_path=eval_path, encoders=one_hot_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist_bw_(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "\n",
    "def manhattan_dist_bw_(v1, v2):\n",
    "    return np.sum(np.abs(v1 - v2))\n",
    "\n",
    "def retrive_closest_n(dists, n=3):\n",
    "    return sorted(dists, key=lambda k: k[1])[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation test: \n",
    "`first_dev_person`s top 3 matches should be 4873, 4788 & 2592 from `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4872, 0.24738633753705963], [4787, 1.4147791347061915], [2591, 1.4156270695349111]]\n",
      "[[4872, 0.30000000000000004], [4787, 2.04], [2591, 2.08]]\n"
     ]
    }
   ],
   "source": [
    "first_dev_person = eval_df[0]\n",
    "eu_dists = pair_wise_dists(vector=first_dev_person, df=train_df, measure=\"euclidean\")\n",
    "mn_dists = pair_wise_dists(vector=first_dev_person, df=train_df, measure=\"man\")\n",
    "\n",
    "print(retrive_closest_n(eu_dists))\n",
    "print(retrive_closest_n(mn_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the five (5) people closest to the last person (in Euclidean distance) in dev, and report their distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_wise_dists(vector, df, measure=\"euclidean\"):\n",
    "    \n",
    "    all_dists = []\n",
    "    for pid, pair_vector in enumerate(df):\n",
    "        \n",
    "        if measure == \"euclidean\":\n",
    "            dist = euclidean_dist_bw_(vector, pair_vector)\n",
    "        else:\n",
    "            dist = manhattan_dist_bw_(vector, pair_vector)    \n",
    "            \n",
    "        all_dists.append([pid, dist])\n",
    "        \n",
    "    return all_dists\n",
    "\n",
    "last_dev_person = eval_df[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1010, 0.05999999999999983],\n",
       " [1713, 0.16000000000000014],\n",
       " [3769, 0.26],\n",
       " [2003, 0.2828427124746192],\n",
       " [2450, 0.3400000000000001]]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eu_dists = pair_wise_dists(vector=last_dev_person, df=train_df, measure=\"euclidean\")\n",
    "retrive_closest_n(eu_dists, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redo the above using Manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1010, 0.05999999999999983],\n",
       " [1713, 0.16000000000000014],\n",
       " [3769, 0.26],\n",
       " [2450, 0.3400000000000001],\n",
       " [2003, 0.40000000000000024]]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_dists = pair_wise_dists(vector=last_dev_person, df=train_df, measure=\"man\")\n",
    "retrive_closest_n(mn_dists, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the 5-NN predictions for this person (Euclidean and Manhattan)? Are these predictions correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth:  <=50K\n",
      "1011th line:  <=50K\n",
      "1714th line:  <=50K\n",
      "2004th line:  <=50K\n",
      "2451th line:  <=50K\n",
      "3770th line:  <=50K\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Ground truth: $(sed -n 1p hw1-data/income.dev.txt | cut -f 10 -d ',')\"\n",
    "\n",
    "echo \"1011th line: $(sed -n 1011p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"1714th line: $(sed -n 1714p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"2004th line: $(sed -n 2004p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"2451th line: $(sed -n 2451p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"3770th line: $(sed -n 3770p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are correct for 5-NN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redo all the above using 9-NN (i.e., find top-9 people closest to this person first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1010, 0.05999999999999983],\n",
       " [1713, 0.16000000000000014],\n",
       " [3769, 0.26],\n",
       " [2003, 0.2828427124746192],\n",
       " [2450, 0.3400000000000001],\n",
       " [3698, 0.4004996878900156],\n",
       " [3680, 0.4386342439892262],\n",
       " [681, 0.5599999999999999],\n",
       " [2731, 1.4142135623730951]]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eu_dists = pair_wise_dists(vector=last_dev_person, df=train_df, measure=\"euclidean\")\n",
    "retrive_closest_n(eu_dists, n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1010, 0.05999999999999983],\n",
       " [1713, 0.16000000000000014],\n",
       " [3769, 0.26],\n",
       " [2450, 0.3400000000000001],\n",
       " [2003, 0.40000000000000024],\n",
       " [3698, 0.41999999999999993],\n",
       " [681, 0.5599999999999999],\n",
       " [3680, 0.58],\n",
       " [2731, 2.0]]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_dists = pair_wise_dists(vector=last_dev_person, df=train_df, measure=\"man\")\n",
    "retrive_closest_n(mn_dists, n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth:  <=50K\n",
      "1011th line:  <=50K\n",
      "1714th line:  <=50K\n",
      "2004th line:  <=50K\n",
      "2451th line:  <=50K\n",
      "3770th line:  <=50K\n",
      "2732th line:  <=50K\n",
      "3681th line:  <=50K\n",
      "682th line:  <=50K\n",
      "3699th line:  <=50K\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Ground truth: $(sed -n 1p hw1-data/income.dev.txt | cut -f 10 -d ',')\"\n",
    "\n",
    "echo \"1011th line: $(sed -n 1011p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"1714th line: $(sed -n 1714p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"2004th line: $(sed -n 2004p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"2451th line: $(sed -n 2451p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"3770th line: $(sed -n 3770p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"2732th line: $(sed -n 2732p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"3681th line: $(sed -n 3681p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"682th line: $(sed -n 682p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\"\n",
    "echo \"3699th line: $(sed -n 3699p hw1-data/income.train.txt.5k | cut -f 10 -d ',')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is correct for 9-NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit2e16e028ce8b4007b970cece9146d16d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
